{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# quantize weights for PTQ\n",
    "def uniform_symmetric_quant(t, b):\n",
    "    tensor = t.detach().numpy()\n",
    "    s = np.max(np.abs(tensor)) / (2**(b-1)-1)\n",
    "    tensor_int = np.round(tensor/s)\n",
    "    tensor_float = tensor_int * s\n",
    "    return torch.tensor(tensor_float)\n",
    "\n",
    "# transfer weights\n",
    "def transfer_weights(quant_model, input_dim, width):\n",
    "    # create new models\n",
    "    model_latent_weights = QuantNet(input_dim=input_dim,width=width,weight_quantizer=None)\n",
    "    model_quant_weights = QuantNet(input_dim=input_dim,width=width,weight_quantizer=None)\n",
    "    \n",
    "    # latent weight model\n",
    "    model_latent_weights.fc1.weight.data = quant_model.fc1.weight.detach()\n",
    "    model_latent_weights.fc1.bias.data = quant_model.fc1.bias.detach()\n",
    "    model_latent_weights.fc2.weight.data = quant_model.fc2.weight.detach()\n",
    "    model_latent_weights.fc2.bias.data = quant_model.fc2.bias.detach()\n",
    "    model_latent_weights.fc3.weight.data = quant_model.fc3.weight.detach()\n",
    "    model_latent_weights.fc3.bias.data = quant_model.fc3.bias.detach()\n",
    "    model_latent_weights.fc4.weight.data = quant_model.fc4.weight.detach()\n",
    "    model_latent_weights.fc4.bias.data = quant_model.fc4.bias.detach()\n",
    "    model_latent_weights.fc5.weight.data = quant_model.fc5.weight.detach()\n",
    "    model_latent_weights.fc5.bias.data = quant_model.fc5.bias.detach()\n",
    "    \n",
    "    # quantized weight model\n",
    "    model_quant_weights.fc1.weight.data = quant_model.fc1.quant_weight().tensor.detach()\n",
    "    model_quant_weights.fc1.bias.data = quant_model.fc1.bias.detach()\n",
    "    model_quant_weights.fc2.weight.data = quant_model.fc2.quant_weight().tensor.detach()\n",
    "    model_quant_weights.fc2.bias.data = quant_model.fc2.bias.detach()\n",
    "    model_quant_weights.fc3.weight.data = quant_model.fc3.quant_weight().tensor.detach()\n",
    "    model_quant_weights.fc3.bias.data = quant_model.fc3.bias.detach()\n",
    "    model_quant_weights.fc4.weight.data = quant_model.fc4.quant_weight().tensor.detach()\n",
    "    model_quant_weights.fc4.bias.data = quant_model.fc4.bias.detach()\n",
    "    model_quant_weights.fc5.weight.data = quant_model.fc5.quant_weight().tensor.detach()\n",
    "    model_quant_weights.fc5.bias.data = quant_model.fc5.bias.detach()\n",
    "    \n",
    "    return model_latent_weights, model_quant_weights\n",
    "\n",
    "def PTQ(baseline_model, b):\n",
    "    baseline_model.fc1.weight.data = uniform_symmetric_quant(baseline_model.fc1.weight, b)\n",
    "    baseline_model.fc2.weight.data = uniform_symmetric_quant(baseline_model.fc2.weight, b)\n",
    "    baseline_model.fc3.weight.data = uniform_symmetric_quant(baseline_model.fc3.weight, b)\n",
    "    baseline_model.fc4.weight.data = uniform_symmetric_quant(baseline_model.fc4.weight, b)\n",
    "    baseline_model.fc5.weight.data = uniform_symmetric_quant(baseline_model.fc5.weight, b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch_cka import CKA\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8WeightPerTensorFloat\n",
    "from brevitas.core.zero_point import ZeroZeroPoint\n",
    "from brevitas.inject.enum import *\n",
    "from src.util.util import train, test, build_dataloaders, QuantNet \n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_experiment(BITS, ROUND, EPOCHS=10, BASELINE=False):\n",
    "    class WeigthQuant(Int8WeightPerTensorFloat):\n",
    "        quant_type = QuantType.INT  # integer quantization\n",
    "        bit_width_impl_type = BitWidthImplType.CONST  # constant bit width\n",
    "        scaling_impl_type = ScalingImplType.STATS  # scale based on statistics\n",
    "        scaling_stats_op = StatsOp.MAX  # scale statistics is the absmax value\n",
    "        restrict_scaling_type = RestrictValueType.FP  # scale factor is a floating point value\n",
    "        scaling_per_output_channel = False  # scale is per tensor\n",
    "        signed = True  # quantization range is signed\n",
    "        narrow_range = True  # quantization range is [-127,127] rather than [-128, 127]\n",
    "        zero_point_impl = ZeroZeroPoint\n",
    "        bit_width = BITS\n",
    "        if ROUND == \"stochastic\":\n",
    "            float_to_int_impl_type = FloatToIntImplType.STOCHASTIC_ROUND\n",
    "        else:\n",
    "            float_to_int_impl_type = FloatToIntImplType.ROUND\n",
    "\n",
    "    if BASELINE:\n",
    "        quantizer = None\n",
    "    else:\n",
    "        quantizer = WeigthQuant\n",
    "\n",
    "    trainloader, testloader = build_dataloaders()\n",
    "\n",
    "    model = QuantNet(input_dim=32*32*3, width=64, weight_quantizer=quantizer)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses, test_losses, l0_norms, l1_norms, l2_norms = train(model=model, optimizer=optimizer, \n",
    "                                                                    criterion=criterion, train_loader=trainloader, \n",
    "                                                                    test_loader=testloader, epochs=EPOCHS, baseline=BASELINE)\n",
    "    # Train a quantized model and create:\n",
    "    # latent weights model and quant weights model\n",
    "    quantized_model = QuantNet(input_dim=32*32*3, width=64, weight_quantizer=quantizer)\n",
    "    optimizer = optim.Adam(quantized_model.parameters(), lr=0.001)\n",
    "    print(\"Training quantized\")\n",
    "    train(model=quantized_model, optimizer=optimizer, criterion=criterion, train_loader=trainloader,\n",
    "          test_loader=testloader, epochs=EPOCHS, baseline=BASELINE)\n",
    "     \n",
    "    model_latent_weights, model_quant_weights = transfer_weights(quantized_model, 32*32*3, 64)\n",
    "    \n",
    "    # train a baseline and create:\n",
    "    # baseline and baseline ptq\n",
    "    baseline_model = QuantNet(input_dim=32*32*3, width=64, weight_quantizer=None)\n",
    "    optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "    print(\"Training baseline\")\n",
    "    train(model=baseline_model, optimizer=optimizer, criterion=criterion, train_loader=trainloader,\n",
    "          test_loader=testloader, epochs=EPOCHS, baseline=BASELINE)\n",
    "    \n",
    "    baseline_model_PTQ = copy.deepcopy(baseline_model)\n",
    "    \n",
    "    # apply PTQ\n",
    "    PTQ(baseline_model_PTQ, BITS)\n",
    "    \n",
    "    # train a second baseline for comparison\n",
    "    baseline_model2 = QuantNet(input_dim=32*32*3, width=64, weight_quantizer=None)\n",
    "    optimizer = optim.Adam(baseline_model2.parameters(), lr=0.001)\n",
    "    print(\"Training baseline 2\")\n",
    "    train(model=baseline_model2, optimizer=optimizer, criterion=criterion, train_loader=trainloader,\n",
    "          test_loader=testloader, epochs=EPOCHS, baseline=BASELINE)\n",
    "    \n",
    "    models = [baseline_model2, model_latent_weights, model_quant_weights]\n",
    "    names = [\"Second Baseline\", \"QAT latent weights\", \"QAT Quant weights\"]\n",
    "    accuracies = {}\n",
    "    for model, name in zip([baseline_model] + models, [\"FP32\"] + names):\n",
    "        accuracy = test(model, criterion, testloader)\n",
    "        accuracies[name] = f'{accuracy:.2f}%'\n",
    "        \n",
    "    # Plot heat map\n",
    "    global_min = 0\n",
    "    global_max = 1\n",
    "    \n",
    "    num_models = len(models)\n",
    "    fig, axs = plt.subplots(1, num_models, figsize=(5 * num_models, 5))\n",
    "    \n",
    "    for idx, (model, name) in enumerate(zip(models, names)):\n",
    "        cka = CKA(baseline_model, model,\n",
    "              model1_name=\"FP32\",\n",
    "              model2_name=name,\n",
    "              model1_layers=['fc1', 'fc2', 'fc3', 'fc4', 'fc5'],\n",
    "              model2_layers=['fc1', 'fc2', 'fc3', 'fc4', 'fc5'],\n",
    "              device='cpu')\n",
    "        cka.compare(testloader, testloader)\n",
    "        results = cka.export()\n",
    "    \n",
    "        ax = axs[idx]\n",
    "        im = ax.imshow(results['CKA'], origin='lower', cmap='magma', vmin=global_min, vmax=global_max)\n",
    "        ax.set_xticks(range(5))\n",
    "        ax.set_yticks(range(5))\n",
    "        ax.set_xticklabels(['1', '2', '3', '4', '5'])\n",
    "        ax.set_yticklabels(['1', '2', '3', '4', '5'])\n",
    "        ax.set_xlabel(f\"Layers {name}\", fontsize=12)\n",
    "        ax.set_ylabel(f\"Layers FP32\", fontsize=12)\n",
    "        ax.set_title(f\"FP32 vs {name}\", fontsize=12) # \\n{accuracies['FP32']} / {accuracies[name]}\n",
    "        ax.grid(False)\n",
    "        # diagonal with CKA values\n",
    "        for i in range(5):\n",
    "            text_color = 'white' if results['CKA'][i, i] < (global_min + global_max) / 2 else 'black'\n",
    "            ax.text(i, i, f'{results[\"CKA\"][i, i]:.2f}', ha='center', va='center', color=text_color)\n",
    "            \n",
    "    if BITS == 2:\n",
    "        fig.suptitle(f'Ternary', fontsize=20)\n",
    "    else:\n",
    "        fig.suptitle(f'{BITS}bit', fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, train_losses, test_losses, l0_norms, l1_norms, l2_norms\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_experiment(2, \"ROUND\", EPOCHS=50, BASELINE=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
